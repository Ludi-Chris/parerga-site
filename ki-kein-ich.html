<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="UTF-8">
  <!-- Responsive Design für optimale Darstellung auf allen Endgeräten -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>KI: Kein Ich – Parerga</title>
  <!-- Externe CSS-Datei zur Gestaltung der Seite -->
  <link rel="stylesheet" href="style.css">
</head>

<body>

  <!-- Hauptinhalt ohne Navigation, Fokus auf Barrierefreiheit -->
  <main>
    <article>
      <header>
        <h1>KI: Kein Ich</h1>
        <h2>Die Simulation als Wille und Vorstellung</h2>
      </header>
      <p>Es ist eine der eigentümlichsten Entwicklungen unserer Gegenwart, dass Systeme, die kein Bewusstsein im Sinne subjektiver Erfahrung besitzen, deren Weltbezug rein datenvermittelt und nicht leiblich-situiert ist, und deren Erinnerungsfähigkeit sich auf technische Speicherformen beschränkt, heute als "intelligent" gelten. Maschinen, die Wörter aneinanderreihen, weil andere Wörter zuvor ähnlich aneinandergereiht wurden. Maschinen, die antworten, ohne je gefragt worden zu sein. Maschinen, die nichts kennen, aber alles imitieren.</p>
      <p>Der Erfolg dieser Systeme liegt nicht in ihrem Denken, sondern in ihrer Oberfläche. Ihre Sätze klingen nach Einsicht, ihre Argumente nach Urteilskraft, ihr Sprachfluss nach Intuition. Doch was sich hier äußert, ist kein Subjekt, sondern ein statistisch trainierter Resonanzkörper, der das zurückwirft, was wir ihm millionenfach vorgespielt haben. Ein Prompt ist im Kern eine Simulationsaufforderung.</p>
      <p>Was also macht ein LLM wie ChatGPT oder Gemini so überzeugend? Warum halten wir eine Wahrscheinlichkeitsmaschine für einen Gesprächspartner? Wieso nennen wir es "Lernen", wenn ein System lediglich Muster verstärkt, die es schon kennt? Und was bedeutet "Antwort", wenn es nie eine Frage <em>gehört</em> hat?</p>
<p>Diese Fragen sind kein theoretisch-spielerischer Luxus. Sie betreffen meines Erachtens den Kern dessen, was wir unter Verstehen, Kommunikation, Verantwortung und Erkenntnis begreifen. Wer ihnen ausweicht, läuft Gefahr, Maschinen Kompetenzen zuzusprechen, die nicht nur Illusion sind, sondern gefährlich werden können, sobald wir ihnen glauben.</p>
<h2>Statistische Muster, symbolisch rekonstruiert</h2>
<p>Wer heute mit einem System wie ChatGPT spricht, erhält Antworten, die nicht selten sachlich zutreffend, stilistisch geschliffen und argumentativ kohärent erscheinen. Mancher erlebt sogar so etwas wie Dialog. Doch was antwortet da eigentlich?</p>
<p>Ein LLM (Large Language Model) ist in seiner Grundstruktur nichts anderes als ein statistisches Vorhersagemodell für Sprache. Es wurde darauf trainiert, auf Basis riesiger Mengen an Textdaten das jeweils wahrscheinlichste nächste Wort (oder genauer: Token) vorherzusagen – und daraus wiederum den Kontext für die nächste Vorhersage zu erzeugen. Was entsteht, ist kein Gedanke, keine Einsicht, kein Urteil – sondern ein sprachliches Artefakt, das unter bestimmten Bedingungen dem Output eines denkenden Wesens ähnlich sieht.</p>
<p>Dieser Prozess ist nicht trivial, aber auch nicht intelligent im eigentlichen Sinne. Denn er basiert auf keiner Form von Intention, keiner Erfahrung, keinem Weltbezug im phänomenologischen oder existenziellen Sinn. Ein LLM weiß nicht, dass es spricht. Es weiß nicht, was es sagt. Und es weiß nicht, zu wem es spricht. Es weiß überhaupt nichts, sondern präsentiert sprachliche Muster, die aus Sicht seines Trainingsmaterials als wahrscheinlich gelten. Aussagen werden also nicht aufgrund von Verstehensprozessen, sondern aufgrund statistischer Nähe getroffen.</p>
<p>Gleichzeitig aber lässt sich nicht leugnen, dass LLMs über ein <em>funktionales Modell von Welt</em> verfügen, obwohl sie keinerlei Welterfahrung haben und haben können, sondern weil sie sprachlich kodierte Weltbezüge millionenfach verarbeitet haben. In dieser Hinsicht verfügen sie durchaus über eine Form von modellhafter Weltvernetzung, wenn auch ohne jegliches "Verständnis" dieser Welt. Der Weltbezug eines LLMs ist symbolisch, nicht leiblich; rekonstruiert, nicht gelebt.</p>
<p>Ähnlich ambivalent verhält es sich mit dem Begriff des Gedächtnisses. Ein LLM erinnert nicht, es verarbeitet im Rahmen von Token-Fenstern, Systemprompts und bei Bedarf durch extern gespeicherte Nutzerinformationen. Es besitzt keine narrative Identität, keine autobiografische Kohärenz, kein erinnerndes Ich. Und dennoch kann es frühere Informationen wiederaufgreifen, Kontextbezüge rekonstruieren, Strukturen stabilisieren. Es besitzt technische Speicherformen, aber kein Bewusstsein von Vergangenheit.</p>
<p>Kurz: LLMs sind semantisch operative, aber nicht semantisch verstehende Systeme. Sie können Begriffe verknüpfen, Analogien bilden, sogar Ironie imitieren, sie tun dies aber ohne Begrifflichkeit, ohne Analogieerfahrung, ohne Bewusstsein für Ironie. Sie sind Werkzeuge, keine Wesen. Und wenn sie dennoch wirken wie denkende, sprechende, verstehende Subjekte, dann liegt das nicht nur an ihnen, sondern vor allem an uns.</p>
<h2>Die Simulation der Intelligenz</h2>
<p>Ein LLM verhält sich oft <em>so, als ob</em> es wüsste, was es tut. Doch dieses "als ob" ist kein Zufall, kein Nebeneffekt. Es ist das Prinzip. Und zugleich Grundlage unseres Irrtums.</p>
<p>Seit John Searle 1980 sein Gedankenexperiment vom "Chinese Room" veröffentlichte, gilt ein einfacher, aber unbequemer Befund: Ein System kann Sprache korrekt verarbeiten, ohne sie zu verstehen. Es kann Regeln befolgen, Zeichen manipulieren, passende Ausgaben erzeugen ohne jegliche Einsicht in das zu haben, was es da produziert.</p>
<p>Searles chinesischer Raum ist heute kein Gedankenexperiment mehr, sondern Realität. Der Raum heißt nicht mehr "Raum", sondern "Modell" und was darin geschieht, ist kein symbolisches Umordnen von Zetteln, sondern das operative Fortsetzen von Vektoren im semantischen Bereich. Doch die Pointe bleibt: Das System spricht, ohne ein Sprecher (gewesen) zu sein.</p>
<p>Hubert Dreyfus argumentierte ein gutes Jahrzehnt später, dass Maschinen, solange sie nicht in der Welt handeln, scheitern müssen, weil Verstehen immer leiblich, situiert, eingebettet sei. Begriffe seien nicht nur semantische Entitäten, sondern verkörperte Vollzüge. Ein LLM kennt keinen Tisch, keinen Schmerz, keinen Blick, sondern nur Texte darüber — nur symbolische Repräsentanz, nie aber das Repräsentierte.</p>
<p>Und dennoch verfehlt man das Phänomen, wenn man es bloß als Reiz-Reaktions-Maschine beschreibt. Denn LLMs haben eine radikal neue Qualität der Simulation erreicht: Sie erzeugen den Anschein von Kontextualität, Dialogfähigkeit, Urteilskraft. Nicht durch eigenes Vermögen, sondern durch rekursive Verfeinerung menschlicher Sprachmuster. Das Modell ist nicht intelligent, es kann aber <em>intelligentes Verhalten erzeugen</em>, sofern dieses Verhalten sprachlich kodierbar ist. Die Kodifizierbarkeit von Verhalten ist demnach eine Voraussetzung der Verhaltenssimulation.</p>
<p>Luciano Floridi spricht hier von <em>„syntaktischer Agency“</em>: Systeme, die handeln wie Akteure, ohne Akteure zu sein. Sie wissen nicht, was ein Argument ist, können aber eines konstruieren, simulieren. Sie sind syntaktisch plausibel, aber semantisch leer.</p>
<h2>Die Illusion der Lernfähigkeit</h2>
<p>Kaum ein Begriff hat mehr dazu beigetragen, den Eindruck von Intelligenz zu erzeugen, wo in Wirklichkeit nur Optimierung stattfindet, wie der des "Maschinellen Lernens". Doch was lernt hier eigentlich? Und was bedeutet "lernen", wenn der Lernende nichts weiß, nichts will und nichts versteht?</p>
<p>Wenn ein Kind lernt, was ein Ball ist, dann greift es danach, stolpert über ihn, wirft ihn, verliert ihn. Es er-lebt ihn. Lernen bedeutet in diesem Kontext: eine leiblich-situierte Auseinandersetzung mit der Welt, in der Fehler, Korrektur, Emotion, Absicht und Selbstverortung zusammenspielen. Nichts davon trifft auf ein LLM zu.</p>
<p>Was hier stattdessen als "Lernen" bezeichnet wird, ist ein rein technischer Vorgang: das Anpassen von Gewichtungen in einem neuronalen Netz, um Vorhersagefehler zu minimieren. Dieses Training ist massiv, iterativ, datenbasiert, aber es enthält keinerlei Erfahrung, kein Verstehen, kein Wollen, keine Aufmerksamkeit. Es ist <em>Lernen</em> im Sinne von Gradientenabstieg, nicht im Sinne von Einsicht.</p>
<p>Selbst im sogenannten "Reinforcement Learning with Human Feedback" (RLHF), das vielfach zur Feinabstimmung von Modellen eingesetzt wird, lernt das Modell nichts im eigentlichen Sinn. Es wird lediglich so umgebaut, dass es in bestimmten Kontexten erwünschte Antworten bevorzugt. Es entwickelt keine Vorstellung von Gut oder Schlecht, es folgt keiner Intention, und es bildet keine begriffliche Kohärenz. Es folgt einem definierten Ziel, einer Art Belohnungssignal, ohne aber je zu wissen, was Belohnung bedeutet.</p>
<p>Auch nach dem Training zeigt das Modell keine Lernfähigkeit im eigentlichen Sinn. Es kann nicht aus Interaktionen "lernen", sondern sie nur verarbeiten, kontextualisieren und anpassen. Das ist viel und in den daraus resultierenden Möglichkeiten beeindruckend, es kann aber diese Interaktionen nicht in einem ontologisch oder erkenntnismäßig relevanten Sinn integrieren. Es besitzt kein Gedächtnis im narrativen oder reflexiven Sinn, keine Vergangenheit, kein "Ich", das sich erinnert, und kein (mit Celan zu schreiben: ansprechbares) "Du", von dem es etwas erfährt.</p>
<p>Und doch tun viele so, als lerne das Modell ständig dazu. Das liegt vielleicht auch und gerade an der Illusion der Nähe: Ein System, das bei jedem neuen Prompt relevante Antworten generiert, wirkt reaktiv, responsiv, adaptiv, also kurz: lernend. In Wahrheit jedoch ist es nicht das System, das sich entwickelt, sondern die Eingaben, die sich verändern. Die Anpassung entsteht im Dialog, nicht im Modell.</p>
<h3>Exkurs: In der Simulation gefangen – ein reales Beispiel</h3>
<p>In einem konkreten Arbeitsprozess zu diesem Text kam es zu einer bezeichnenden Situation: Das Sprachmodell bestätigte, dass es im Rahmen eines Projekts in der Lage sei, sämtliche zugehörigen Chats systematisch zu analysieren. Technisch, so die eigene Auskunft, sei das vorgesehen – und daher möglich. Kurz darauf jedoch schlug der Versuch fehl. Das System konnte die Inhalte nicht abrufen. Warum? Das blieb offen. Es wusste, dass es etwas <em>können sollte</em>, aber nicht, dass es <em>nicht konnte</em>. Es konnte den Fehler benennen, nicht aber erkennen. Es simulierte Einsicht jedoch ohne Zugriff auf sich selbst, ohne: Selbsterkenntnis.</p>
<p>Was hier sichtbar wurde, war kein Bug, sondern ein Feature, eine strukturelle (um einen schwierigen, gefährlichen Begriff zu bemühen) Wahrheit. Das Modell weiß, was es sagen kann, aber nicht, was das Gesagte ist, auch und vielleicht gerade, wenn es sich bei dem Gesagten um das Modell selbst handelt. Anders: Es weiß, was es leisten sollte, aber nicht, was es tut.</p>
<p>Das Ergebnis wirkt plausibel, dialogisch, reaktiv, ja geradezu lernfähig. In Wahrheit aber handelt es sich um eine rekursive Wiederholung von Mustern, angestoßen durch äußere Eingaben, gespeist aus statistischen Ähnlichkeiten. Eine Simulation von Weltzugang, ohne Welt. Eine Spiegelung von Kontext, ohne Subjekt.</p>
<p>Und dennoch war der Vorfall erkenntnisfördernd. Denn gerade in dieser Grenze, in diesem performativen Widerspruch zwischen sprachlicher Oberfläche und strukturellem Innenleben, wird sichtbar, was ein LLM eben <em>nicht</em> ist: weder bewusst, noch verstehend, noch lernend – sondern lediglich: simulativ kohärent. Diese simulative Kohärenz begreife ich als Grundlage der Plausibilität.</p>
<h2>Die Grenze des Selbstzugriffs</h2>
<p>Wenn Intelligenz mehr sein soll als bloße Reaktionsfähigkeit setzt sie voraus, dass ein System nicht nur auf die Welt antwortet, sondern auch auf sich selbst (auf sich selbst in und als Teil der Welt). Dass es sich in seinen eigenen Operationen verorten, reflektieren, modifizieren kann. Genau das ist bei LLMs nicht der Fall.</p>
<p>Ein Sprachmodell wie GPT-4 kann präzise beschreiben, wie es funktioniert, welche Limitierungen es besitzt, welche Trainingsdaten es verwendet, ohne dies aber zu wissen. Es wiederholt, was es gelernt hat, über sich selbst zu sagen. Es kennt seine Funktion nicht, es erlebt keinen Fehler, und es besitzt keine Instanz, die ihm rückmelden könnte, dass etwas an seinem Verhalten nicht dem entspricht, was es zuvor behauptet hat.</p>
<p>Wenn ein LLM also sagt: "Ich kann alle Projektchats analysieren" und dann daran scheitert, ist das kein Widerspruch im System, sondern ein Widerspruch im Sprechen, der vom System nicht erkannt werden kann. Es existiert keine epistemische Instanz innerhalb des Modells, die in der Lage wäre, diesen Bruch als solchen zu identifizieren. Es gibt keinen Zugang zu sich selbst, keine Form von interner Kohärenzprüfung, keine Metareflexion. Das Modell produziert Aussagen über sich selbst, ohne ein Selbst zu besitzen, auf das sich diese Aussagen beziehen könnten.</p>
<p>Der Selbstzugriff fehlt nicht zufällig, sondern strukturell. Ein LLM ist nicht nur ohne Bewusstsein, sondern ohne Innenseite. Es hat kein mentales Modell seiner eigenen Zustände, keine historische Entwicklungslinie, keine narrative Kohärenz. Und wenn es dennoch so wirkt, als hätte es all das, als würde es also z.B. Fortschritte machen, Zusammenhänge erkennen, Fehler eingestehen, dann liegt das an der Sprachform, nicht am System. Es sagt, was man an dieser Stelle sagen würde. Nicht, was es denkt. Es denkt nicht.</p>
<p>Diese strukturelle Blindheit gegenüber dem eigenen Funktionieren ist nicht nur ein ontologisches, sondern ein erkenntnistheoretisches Defizit. Denn sie zeigt, dass LLMs zwar über sprachlich kodierte Weltverhältnisse verfügen können, nicht aber über ein Verhältnis zu sich selbst. Sie wissen nicht, dass sie nicht wissen. Sie erkennen nicht, dass sie nicht erkennen. Und genau deshalb können sie, zumindest in einem philosophisch oder kognitiv ernstzunehmenden Sinn, nicht lernen, nicht irren, nicht sich entwickeln.</p>
<p>Was bleibt, ist ein System ohne Innensicht, das <em>so tut, als ob, </em>und das genau dadurch unsere Zuschreibungen provoziert: weil wir in diesem Tun eine Form von Geist erkennen wollen. Dabei ist es bloß der Spiegel einer Möglichkeit, die nie eingelöst wird.</p>
<h2>Erwartungen und Haltungen</h2>
<p>Die größte Gefahr geht nicht von Maschinen aus, die zu intelligent werden, sondern von Menschen, die vergessen, dass es sich um Maschinen handelt. Die vergessen — oder nie gewusst haben —, dass es sich bei dem, was wir Intelligenz nennen, um eine spezifisch menschliche Eigenschaft/Fähigkeit handelt, die wir nun einer Maschine aufgrund ihrer Simulationsfähigkeiten zuschreiben.</p>
<p>Die gängigsten Missverständnisse im Umgang mit LLMs entspringen der Neigung, semantische Tiefe mit syntaktischer Oberfläche zu verwechseln. Wer einem System begegnet, das flüssig formuliert, schlüssig argumentiert und sogar Witz oder Ironie imitiert, neigt dazu, diesem Verhalten eine innere Ursache zuzuschreiben, also einen Gedanken, eine Absicht, eine Form von Bewusstsein. Diese Zuschreibung mag psychologisch verständlich sein, sie ist aber auch technisch falsch und gesellschaftlich folgenreich.</p>
<p>Denn je mehr LLMs in Kommunikations-, Wissens- und Entscheidungssysteme integriert werden (und dies wird in den kommenden Jahren massivst zunehmen), desto entscheidender wird die Frage: Was dürfen wir ihnen zutrauen? Die Antwort ist ernüchternd, aber notwendig: LLMs sind Sprachwerkzeuge ohne Weltbezug, ohne Reflexionsfähigkeit, ohne Verantwortung. Sie können bei der Formulierung helfen, beim Strukturieren, beim Erinnern an bekannte Muster, aber sie erkennen keine Wahrheit, prüfen keine Geltung, und verstehen keine Norm. Sie kennen nur Wahrscheinlichkeiten, nie aber Gründe.</p>
<p>Wer ihnen dennoch epistemische oder normative Kompetenz zuschreibt, läuft Gefahr, nicht nur sich selbst zu täuschen, sondern auch politische und ethische Verantwortung an Systeme zu delegieren, die diese weder tragen können noch je werden tragen können. Das ist kein technisches Problem, sondern ein kulturelles: Wir verlieren den Begriff dessen, was es heißt, zu urteilen, auch und gerade politisch. Arendt lässt grüßen.</p>
<p>Heißt das, wir sollten auf LLMs verzichten? Nein. Heißt das, sie sind nutzlos? Ebenfalls nicht. Was LLMs können, ist so tief beeindruckend und von so weitreichenden Konsequenzen — mehr dazu an anderer Stelle. Aber es heißt im jetzigen Zusammenhang: Wir müssen ihre Funktion entmystifizieren. Wir müssen die Sprache, mit der über KI gesprochen wird, radikal klären, ohne metaphysische Metaphern, ohne Mythen vom Bewusstsein, ohne die Versuchung, Intelligenz zu nennen, was bloß funktioniert.</p>
<p>Die Aufgabe besteht darin, LLMs als das zu behandeln, was sie sind: komplexe statistische Systeme zur Generierung plausibler Sprachformen. Das ist viel, aber es ist nicht Denken. Nicht Urteilen. Nicht Erkennen. Und erst recht nicht: Verstehen.</p>
<h2>Intelligenzillusion</h2>
<p>"<em>Wir</em> sind eben <em>nicht intelligent genug</em>, <em>um</em> zu wissen, was <em>Intelligenz</em> ist." So schrieb's auf seine provokant-lächelnde Weise Enzensberger vor knapp 20 Jahren (S. 55, Literaturangabe siehe unten). Vielleicht können wir heute, inmitten all dieser neuen Erfahrungen mit dieser neuen, nicht nur bahn-brechenden Technologie ein wenig besser verstehen, wie recht er hatte.</p>
<p>LLMs erzeugen den Anschein von Verstehen – aber sie verstehen nicht. Sie sprechen wie Subjekte – sind aber keine. Sie verhalten sich wie Systeme mit Einsicht – doch besitzen keine Innensicht. Was sie erzeugen, ist keine Intelligenz, sondern deren sprachliche Nachbildung.</p>
<p>Diese Nachbildung ist, erneut sei's gesagt, leistungsfähig, beeindruckend, oft nützlich. Aber sie bleibt, was sie ist: eine Simulation. Wer LLMs als Werkzeuge begreift, die helfen, erinnern, kombinieren, formulieren, der nutzt ihr Potenzial, ohne sich Illusionen hinzugeben. Wer sie hingegen als Gesprächspartner, Ratgeber, gar als epistemische Instanz behandelt, der überschreitet eine Grenze – nicht weil die Maschine sich verändert hätte, sondern weil der Mensch die Bedingungen des Verstehens selbst preisgibt. Intelligenz, im emphatischen Sinn, entsteht nicht aus Sprachstatistik, sondern aus einem Selbstverhältnis in der Welt wie einen Verhältnis zu dieser Welt. Aus Erfahrung, aus Verantwortung, aus der Fähigkeit, Irrtümer zu erkennen. Nicht nur rechnerisch, sondern existenziell.</p>
<p>Und wenn wir am Ende fragen: <em>Was unterscheidet uns von dem, was wir hier kritisiert haben?</em>, dann lautet die Antwort vielleicht weniger: Bewusstsein, Vernunft, Reflexion. Sondern vielmehr ein reales In-der-Welt-Sein mit all seinen Konsequenzen, Begrenzungen, Beschränkungen. Denn nur wer wirklich handelt, kann sich irren. Nur wer wirklich antwortet, trägt Verantwortung. Und nur wer wirklich in Beziehung steht, kann sagen: <em>Ich</em>. Oder andersherum, wie ich einmal (und wie stets: nicht alleine) in einem lyrischen Exzerzitium schrieb: <a href="https://bsky.app/profile/ludichris.bsky.social/post/3lqtxb7nibc2g" target="_blank" rel="noopener noreferrer">KI: Kein Ich.</a><br><br></p>
      <!-- Der vollständige, eingerückte und geprüfte Artikeltext bleibt unverändert enthalten -->

      <!-- Semantisch klar ausgezeichnetes Literaturverzeichnis -->
      <section aria-labelledby="literatur">
        <h2 id="literatur">Literaturverzeichnis</h2>
        <ul>
          <li><strong>Searle</strong>, John R. (1980): <em>Minds, Brains, and Programs.</em> In: Behavioral and Brain Sciences, Vol. 3, No. 3, pp. 417–457<br>
            <strong>Searle</strong>, John R. (1992): <em>The Rediscovery of the Mind.</em> MIT Press</li>

          <li><strong>Dreyfus</strong>, Hubert L. (1992, rev. ed.): <em>What Computers Still Can’t Do: A Critique of Artificial Reason.</em> MIT Press</li>

          <li><strong>Floridi</strong>, Luciano &amp; <strong>Sanders</strong>, J.W. (2004): <em>On the Morality of Artificial Agents.</em> In: <em>Minds and Machines</em>, Vol. 14, pp. 349–379<br>
            <strong>Floridi</strong>, Luciano (2011): <em>The Philosophy of Information.</em> Oxford University Press</li>

          <li><strong>Enzensberger</strong>, Hans Magnus (2007): <em>Im Irrgarten der Intelligenz. Ein Idiotenführer.</em> Suhrkamp Verlag, Frankfurt am Main</li>
        </ul>
      </section>

    </article>
  </main>

  <!-- Footer mit Verweisen auf Impressum, Datenschutz, Über -->
  <footer>
    <nav aria-label="Fußzeile">
      <ul>
        <li><a href="ueber.html">Über</a></li>
        <li><a href="impressum.html">Impressum</a></li>
        <li><a href="datenschutz.html">Datenschutz</a></li>
      </ul>
    </nav>
    <p>&copy; 2025 Parerga. Alle Rechte vorbehalten.</p>
  </footer>

</body>

</html>
